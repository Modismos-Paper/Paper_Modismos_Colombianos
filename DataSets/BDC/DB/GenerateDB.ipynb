{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7bdf72",
   "metadata": {},
   "source": [
    "# Dataset Caro y Cuervo\n",
    "\n",
    "En este notebook se va a limpiar y preprocesar el resultado en markdown de la herramienta Marker para consolidar el dataset de BDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211428aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "path_source = \"Diccionario_breve_de_Colombiaismos_slown_text.txt\"\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES BÁSICAS DE NORMALIZACIÓN Y LIMPIEZA DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "def nfc(s: str) -> str:\n",
    "    \"\"\"Normaliza texto a forma canónica de composición Unicode (NFC).\"\"\"\n",
    "    return unicodedata.normalize(\"NFC\", s or \"\")\n",
    "\n",
    "def norm_spaces(s: str) -> str:\n",
    "    \"\"\"Reemplaza espacios no separables y de ancho cero, y normaliza múltiples espacios.\"\"\"\n",
    "    s = (s or \"\").replace(\"\\u00A0\", \" \").replace(\"\\u200b\", \"\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "st_end_punt_re = re.compile(r\"^[¡!¿?«»\\\"'()\\[\\]{}·•.,;:—–-]+|[¡!¿?«»\\\"'()\\[\\]{}·•.,;:—–-]+$\")\n",
    "def strip_punct(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina signos de puntuación ubicados al inicio o al final de la cadena.\n",
    "    No afecta la puntuación que está en el medio.\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    Entrada: \"**¡ahijuelita!**\"\n",
    "    Salida:  \"ahijuelita\"\n",
    "    \"\"\"\n",
    "    return st_end_punt_re.sub(\"\", s or \"\").strip()\n",
    "\n",
    "\n",
    "def clean_curs(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Quita el marcado de negrita y cursiva en Markdown, dejando solo el texto limpio.\n",
    "\n",
    "    - Elimina `**` (negrita)\n",
    "    - Elimina `__` (subrayado)\n",
    "    - Reemplaza `*palabra*` por `palabra` (estaba en cursiva)\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    Entrada: \"**¡ahijuelita!**  *bonita*\"\n",
    "    Salida:  \"¡ahijuelita!  bonita\"\n",
    "    \"\"\"\n",
    "    s = (s or \"\").replace(\"**\", \"\").replace(\"__\", \"\")\n",
    "    s = re.sub(r\"\\*(.*?)\\*\", r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "remove_trailing_pipes_pattern = re.compile(r\"\\s*\\|+\\s*$\")\n",
    "\n",
    "def remove_trailing_pipes(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina las barras verticales '|' que aparecen al final de la cadena.\n",
    "    También remueve los espacios que puedan estar antes de las barras.\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    Entrada: \"ser vivo ||\"\n",
    "    Salida:  \"ser vivo\"\n",
    "\n",
    "    Entrada: \"abeja |\"\n",
    "    Salida:  \"abeja\"\n",
    "    \"\"\"\n",
    "    return remove_trailing_pipes_pattern.sub(\"\", s or \"\").strip()\n",
    "\n",
    "remove_gender_suffix_pattern = re.compile(r\"\\s*,\\s*(?:a|da)\\b\\.?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def remove_gender_suffix(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Quita el sufijo literal \", a\" o \", da\" al final de un lema para quedarse con la forma base.\n",
    "    Si no encuentra el sufijo, deja el texto sin cambios.\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    Entrada: \"bacano, a\"\n",
    "    Salida:  \"bacano\"\n",
    "\n",
    "    Entrada: \"cansado, da\"\n",
    "    Salida:  \"cansado\"\n",
    "\n",
    "    Entrada: \"árbol\"\n",
    "    Salida:  \"árbol\"\n",
    "    \"\"\"\n",
    "    before = token\n",
    "    after = remove_gender_suffix_pattern.sub(\"\", token)\n",
    "    return (after.strip() or before.strip())\n",
    "\n",
    "# ============================================================================\n",
    "# EXPANSIÓN DE TILDES (~) EN FRASES DERIVADAS\n",
    "# ============================================================================\n",
    "\n",
    "def replace_tilde_with_base(text: str, base: str) -> str:\n",
    "    \"\"\"\n",
    "    Reemplaza la tilde \"~\" en una frase derivada por la palabra base del lema.\n",
    "    Mantiene correctamente sufijos, espacios y signos de puntuación.\n",
    "\n",
    "    Ejemplos:\n",
    "\n",
    "    - Entrada: texto=\"~ de encostalados\", palabra_base=\"carrera\"\n",
    "      Salida:  \"carrera de encostalados\"\n",
    "\n",
    "    - Entrada: texto=\"echar ~\", palabra_base=\"carreta\"\n",
    "      Salida:  \"echar carreta\"\n",
    "\n",
    "    - Entrada: texto=\"hablar ~.\", palabra_base=\"carreta\"\n",
    "      Salida:  \"hablar carreta.\"\n",
    "\n",
    "    - Entrada: texto=\"~osa\", palabra_base=\"carranch\"\n",
    "      Salida:  \"carranchosa\"\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    # ~ seguida de letras (sufijos)\n",
    "    text = re.sub(r\"~([a-záéíóúüñ]+)\", lambda m: base + m.group(1), text, flags=re.IGNORECASE)\n",
    "    # ~ seguida de espacios y letras\n",
    "    text = re.sub(r\"~(\\s+)([a-záéíóúüñ]+)\", lambda m: base + m.group(1) + m.group(2), text, flags=re.IGNORECASE)\n",
    "    # ~ seguida de puntuación\n",
    "    text = re.sub(r\"~([.,;:?!])\", lambda m: base + m.group(1), text)\n",
    "    # ~ restantes\n",
    "    text = text.replace(\"~\", base)\n",
    "    return text\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESAMIENTO DE LEMAS CON PIPES (||) Y TILDE (~)\n",
    "# ============================================================================\n",
    "\n",
    "def resolve_pipe_tilde_in_head(head_bold_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Resuelve el caso de encabezados que usan \"||\" (pipes) y tilde \"~\".\n",
    "    Toma la parte DERECHA del \"||\" y reemplaza \"~\" por la palabra base que está a la izquierda.\n",
    "\n",
    "    Ejemplos:\n",
    "\n",
    "    - Entrada: lema_con_barras = \"carrera || ~ de encostalados\"\n",
    "      Salida:  \"carrera de encostalados\"\n",
    "\n",
    "    - Entrada: lema_con_barras = \"carraca || echar ~\"\n",
    "      Salida:  \"echar carraca\"\n",
    "\n",
    "    - Entrada: lema_con_barras = \"carreta || echar o hablar ~\"\n",
    "      Salida:  \"echar o hablar carreta\"\n",
    "    \"\"\"\n",
    "    parts = head_bold_content.split(\"||\", 1)\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    left, right = parts[0], parts[1]\n",
    "    base = strip_punct(clean_curs(norm_spaces(left)))\n",
    "    base = remove_gender_suffix(base)\n",
    "    base = strip_punct(base)\n",
    "    rhs = norm_spaces(right)\n",
    "    rhs = replace_tilde_with_base(rhs, base)\n",
    "    rhs = strip_punct(rhs)\n",
    "    rhs = remove_trailing_pipes(rhs)\n",
    "    rhs = norm_spaces(rhs)\n",
    "    return rhs\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACCIÓN Y NORMALIZACIÓN DE LEMAS\n",
    "# ============================================================================\n",
    "\n",
    "re_bold_head = re.compile(r\"^-?\\s*\\*\\*(.*?)\\*\\*\")\n",
    "\n",
    "def extract_bold_head_general(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae el lema (palabra principal) que aparece encerrado entre **negritas** \n",
    "    al inicio de una línea en el diccionario.\n",
    "\n",
    "    Ejemplos:\n",
    "\n",
    "    - Entrada: linea = \"- **carraca** f. coloq. Mandíbula del hombre o los animales.\"\n",
    "      Salida:  \"carraca\"\n",
    "\n",
    "    - Entrada: linea = \"- **carreta || echar o hablar ~** fr. coloq. Hablar cosas triviales...\"\n",
    "      Salida:  \"carreta || echar o hablar ~\"\n",
    "\n",
    "    - Entrada: linea = \"- **carranchil (carranchín)** m. Enfermedad cutánea caracterizada por un fuerte escozor.\"\n",
    "      Salida:  \"carranchil (carranchín)\"\n",
    "\n",
    "    - Entrada: linea = \"- **carrera || ~ de encostalados** Competencia deportiva en la cual los participantes...\"\n",
    "      Salida:  \"carrera || ~ de encostalados\"\n",
    "    \"\"\"\n",
    "    m = re_bold_head.search(line)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def remove_optional_plural_suffix(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina sufijos de plural opcional \"(s)\" o \"(es)\" de un lema.\n",
    "\n",
    "    Esto es común en el diccionario cuando un lema admite plural, \n",
    "    pero no se quiere que aparezca como parte de la palabra base.\n",
    "\n",
    "    Ejemplos :\n",
    "    \n",
    "    - Entrada: \"botarata(s)\"  \n",
    "      Salida:  \"botarata\"\n",
    "\n",
    "    - Entrada: \"carriel(es)\"  \n",
    "      Salida:  \"carriel\"\n",
    "\n",
    "    - Entrada: \"mataburro(s)\"  \n",
    "      Salida:  \"mataburro\"\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\((?:s|es)\\)$\", \"\", token, flags=re.IGNORECASE)\n",
    "\n",
    "def normalize_headword_content(head_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza el lema extraído de la cabecera en **negrita** de un artículo.\n",
    "\n",
    "    Acciones realizadas:\n",
    "    1. Normaliza caracteres Unicode a forma NFC.\n",
    "    2. Si el lema contiene '||' (derivados con \"~\"), los resuelve\n",
    "       reemplazando \"~\" por la palabra base.\n",
    "    3. Elimina sufijos de plural opcional \"(s)\" o \"(es)\".\n",
    "    4. Descarta variantes indicadas entre paréntesis después del lema.\n",
    "    5. Elimina colas literales como \", a\" o \", da\" (marcas de género).\n",
    "    6. Limpia signos de puntuación y barras verticales sobrantes.\n",
    "    7. Normaliza espacios en blanco.\n",
    "\n",
    "    Ejemplos :\n",
    "    \n",
    "    - Entrada: \"carranchil (carranchín)\"\n",
    "      Salida:  \"carranchil\"\n",
    "\n",
    "    - Entrada: \"carrera || ~ de encostalados\"\n",
    "      Salida:  \"carrera de encostalados\"\n",
    "\n",
    "    - Entrada: \"botarata(s)\"\n",
    "      Salida:  \"botarata\"\n",
    "    \"\"\"\n",
    "    head = nfc(head_content)\n",
    "    if \"||\" in head:\n",
    "        resolved = resolve_pipe_tilde_in_head(head)\n",
    "        if resolved:\n",
    "            return nfc(remove_optional_plural_suffix(resolved))\n",
    "    token = norm_spaces(head)\n",
    "    token = remove_optional_plural_suffix(token)\n",
    "    token = token.split(\" (\", 1)[0]\n",
    "    token = remove_gender_suffix(token)\n",
    "    token = strip_punct(token)\n",
    "    token = remove_trailing_pipes(token)\n",
    "    token = norm_spaces(token)\n",
    "    return nfc(token)\n",
    "\n",
    "# ============================================================================\n",
    "# PATRONES Y CONFIGURACIÓN DE LIMPIEZA DE METADATOS\n",
    "# ============================================================================\n",
    "\n",
    "# Detecta ocurrencias externas: \"|| **frase_con_tilde** resto_del_texto\"\n",
    "external_occurrence_pattern = re.compile(\n",
    "    r\"\\|\\|\\s*\\*{2}\\s*(.*?)\\s*\\*{2}\\s*([^|]*)(?=(?:\\|\\||$))\",\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "\n",
    "# Etiquetas gramaticales a eliminar\n",
    "grammatical_tags = [\n",
    "    \"m\", \"f\", \"pl\", \"sing\", \"sust\",\n",
    "    \"adj\", \"adv\", \"intr\", \"tr\", \"prnl\", \"fr\", \"interj\",\n",
    "    \"coloq\", \"pop\", \"rur\", \"vulg\", \"despect\", \"obsol\"\n",
    "]\n",
    "\n",
    "gender_pattern = r\"(?:m(?:\\s*\\.)?\\s*(?:y\\s*f(?:\\s*\\.)?)?)\"\n",
    "utc_pattern_PATTERN = r\"U\\s*\\.\\s*t\\s*\\.\\s*c\\s*\\.\\s*(?:s|adj|prnl)\\s*\\.?\"\n",
    "\n",
    "# Patrón compuesto para eliminar etiquetas gramaticales\n",
    "drop_tag = re.compile(\n",
    "    r\"(?:\\b{mf}\\b)|(?:\\b(?:{base})(?:\\s*\\.)?\\b)|(?:\\b{utc_pattern}\\b)\".format(\n",
    "        mf=gender_pattern, base=\"|\".join(grammatical_tags), utc_pattern=utc_pattern_PATTERN\n",
    "    ),\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Nombres de regiones colombianas (ordenadas de más largo a más corto para evitar coincidencias parciales)\n",
    "region_names = [\n",
    "    \"Costa del Pacífico\",\"Costa del Pacifico\",\"Costa Atlántica\",\"Costa Atlantica\",\n",
    "    \"Norte de Santander\",\"Valle del Cauca\",\"Llanos Orientales\",\n",
    "    \"Costa Pacíf\",\"Costa Pacif\",\"Costa Atl\",\"La Guajira\",\n",
    "    \"Cundinamarca\",\"Atlántico\",\"Atlantico\",\"Antioquia\",\"Magdalena\",\"Risaralda\",\n",
    "    \"Santander\",\"Putumayo\",\"Caquetá\",\"Caqueta\",\"Casanare\",\"Córdoba\",\"Cordoba\",\n",
    "    \"Bolívar\",\"Bolivar\",\"Boyacá\",\"Boyaca\",\"Nariño\",\"Narino\",\"Quindío\",\"Quindio\",\n",
    "    \"Amazonas\",\"Caldas\",\"Bogotá\",\"Bogota\",\"Chocó\",\"Choco\",\"Tolima\",\"Cauca\",\"Valle\",\"Huila\",\"Meta\",\"Arauca\",\"Llanos\",\n",
    "    \"NStder\",\"Amaz\",\"Stder\",\"Cund\",\"Córd\",\"Cord\",\"Quind\",\"Risar\",\"Magd\",\"Guaj\",\"Tol\",\"Ant\",\"Atl\",\"Bog\",\"Bol\",\"Boy\",\"Cald\",\"Nar\"\n",
    "]\n",
    "\n",
    "# Mapeo de abreviaturas a nombres completos de regiones\n",
    "region_full_names = {\n",
    "    \"amaz\": \"Amazonas\",\n",
    "    \"amazonas\": \"Amazonas\",\n",
    "    \"ant\": \"Antioquia\",\n",
    "    \"antioquia\": \"Antioquia\",\n",
    "    \"atl\": \"Atlántico\",\n",
    "    \"atlántico\": \"Atlántico\",\n",
    "    \"atlantico\": \"Atlántico\",\n",
    "    \"bog\": \"Bogotá\",\n",
    "    \"bogotá\": \"Bogotá\",\n",
    "    \"bogota\": \"Bogotá\",\n",
    "    \"bol\": \"Bolívar\",\n",
    "    \"bolívar\": \"Bolívar\",\n",
    "    \"bolivar\": \"Bolívar\",\n",
    "    \"boy\": \"Boyacá\",\n",
    "    \"boyacá\": \"Boyacá\",\n",
    "    \"boyaca\": \"Boyacá\",\n",
    "    \"cald\": \"Caldas\",\n",
    "    \"caldas\": \"Caldas\",\n",
    "    \"chocó\": \"Chocó\",\n",
    "    \"choco\": \"Chocó\",\n",
    "    \"córd\": \"Córdoba\",\n",
    "    \"cord\": \"Córdoba\",\n",
    "    \"córdoba\": \"Córdoba\",\n",
    "    \"cordoba\": \"Córdoba\",\n",
    "    \"costa atl\": \"Costa Atlántica\",\n",
    "    \"costa atlántica\": \"Costa Atlántica\",\n",
    "    \"costa atlantica\": \"Costa Atlántica\",\n",
    "    \"costa pacíf\": \"Costa del Pacífico\",\n",
    "    \"costa pacif\": \"Costa del Pacífico\",\n",
    "    \"costa del pacífico\": \"Costa del Pacífico\",\n",
    "    \"costa del pacifico\": \"Costa del Pacífico\",\n",
    "    \"cund\": \"Cundinamarca\",\n",
    "    \"cundinamarca\": \"Cundinamarca\",\n",
    "    \"guaj\": \"La Guajira\",\n",
    "    \"la guajira\": \"La Guajira\",\n",
    "    \"llanos\": \"Llanos Orientales\",\n",
    "    \"magd\": \"Magdalena\",\n",
    "    \"magdalena\": \"Magdalena\",\n",
    "    \"nar\": \"Nariño\",\n",
    "    \"nariño\": \"Nariño\",\n",
    "    \"narino\": \"Nariño\",\n",
    "    \"nstder\": \"Norte de Santander\",\n",
    "    \"norte de santander\": \"Norte de Santander\",\n",
    "    \"quind\": \"Quindío\",\n",
    "    \"quindío\": \"Quindío\",\n",
    "    \"quindio\": \"Quindío\",\n",
    "    \"risar\": \"Risaralda\",\n",
    "    \"risaralda\": \"Risaralda\",\n",
    "    \"stder\": \"Santander\",\n",
    "    \"santander\": \"Santander\",\n",
    "    \"tol\": \"Tolima\",\n",
    "    \"tolima\": \"Tolima\",\n",
    "    \"valle\": \"Valle del Cauca\",\n",
    "    \"valle del cauca\": \"Valle del Cauca\",\n",
    "    \"cauca\": \"Cauca\",\n",
    "    \"huila\": \"Huila\",\n",
    "    \"meta\": \"Meta\",\n",
    "    \"arauca\": \"Arauca\",\n",
    "    \"casanare\": \"Casanare\",\n",
    "    \"putumayo\": \"Putumayo\",\n",
    "    \"caquetá\": \"Caquetá\",\n",
    "    \"caqueta\": \"Caquetá\"\n",
    "}\n",
    "\n",
    "region_pattern_union = \"|\".join(map(re.escape, region_names))\n",
    "\n",
    "# Detecta bloques de regiones SOLO en cursiva (entre asteriscos)\n",
    "# Previene eliminar texto normal como \"nar\" dentro de \"nariz\"\n",
    "italic_region_block = re.compile(\n",
    "    r\"\\*\\s*(?:(?:{REG})(?:\\.|\\b))(?:\\s*[,you]\\s*(?:(?:{REG})(?:\\.|\\b)))*\\s*\\*\".format(REG=region_pattern_union),\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Patrones para detectar citas bibliográficas\n",
    "citation_w_paren_re = re.compile(\n",
    "    r'(?:-\\s*)?\\(\\s*(?:Carrasquilla|Le[oó]n Rey)\\s*,\\s*[IVXLCDM]+\\s*(?:,\\s*(?:copla\\s*)?\\d+)?\\s*\"?\\s*\\)?',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "citation_w_quote_re = re.compile(\n",
    "    r'(?:-\\s*)?[\\\"\"\"]\\s*Le[oó]n Rey\\s*,\\s*[IVXLCDM]+\\s*[\\\"\"\"]',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES DE LIMPIEZA DE METADATOS Y EXTRACCIÓN DE REGIONES\n",
    "# ============================================================================\n",
    "\n",
    "def remove_citations(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina citas de obras/autor incrustadas en el texto con los formatos detectados.\n",
    "\n",
    "    Borra patrones como:\n",
    "    - (Carrasquilla, I, 150\")\n",
    "    - (Carrasquilla, II, 291\")\n",
    "    - (León Rey, II, copla 3932\")\n",
    "    - \"León Rey, I\"  /  \"León Rey, I\"\n",
    "\n",
    "    Además, corrige espacios dobles y comas sobrantes antes de signos.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    out = citation_w_paren_re.sub(\"\", text)\n",
    "    out = citation_w_quote_re.sub(\"\", out)\n",
    "    out = re.sub(r\"\\s*,\\s*(?=[.,;:])\", \"\", out)\n",
    "    out = re.sub(r\"\\s{2,}\", \" \", out)\n",
    "    return norm_spaces(out)\n",
    "\n",
    "def extract_regions_from_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extrae las regiones que están EN CURSIVA (*...*) del texto.\n",
    "    Retorna una lista de nombres completos de regiones.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    regions_found = []\n",
    "    for match in italic_region_block.finditer(text):\n",
    "        block = match.group(0)\n",
    "        block_clean = block.strip(\"*\").strip()\n",
    "        # Separar por comas o conjunciones (y, o, u)\n",
    "        parts = re.split(r'\\s*,\\s*|\\s+(?:y|o|u)\\s+', block_clean, flags=re.IGNORECASE)\n",
    "        for part in parts:\n",
    "            part = part.strip().rstrip(\".\").strip()\n",
    "            if not part:\n",
    "                continue\n",
    "            part_lower = part.lower()\n",
    "            full_name = region_full_names.get(part_lower, None)\n",
    "            # Intentar sin acentos si no se encuentra\n",
    "            if full_name is None:\n",
    "                part_no_accent = part_lower.replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\", \"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\")\n",
    "                full_name = region_full_names.get(part_no_accent, None)\n",
    "            \n",
    "            # Usar valor original con capitalización si no se encuentra en el diccionario\n",
    "            if full_name is None:\n",
    "                full_name = part.title()\n",
    "            \n",
    "            if full_name and full_name not in regions_found:\n",
    "                regions_found.append(full_name)\n",
    "    \n",
    "    return regions_found\n",
    "\n",
    "def clean_regional_blocks_and_grammar_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia del texto:\n",
    "    1) Bloques de regiones SOLO cuando están en cursiva *...* \n",
    "       (p. ej., *Cund., Boy., Nar.*; *Ant., Cald., Valle.*).\n",
    "    2) Abreviaturas/etiquetas gramaticales con límites de palabra\n",
    "       (m., f., adj., intr., tr., prnl., fr., interj., coloq., etc., y \"U. t. c. ...\").\n",
    "    3) Citas bibliográficas mediante `strip_bibliographic_citations`.\n",
    "    4) Puntuación/espacios sobrantes tras las limpiezas.\n",
    "\n",
    "    - No afecta texto normal: no elimina \"nar\" dentro de \"nariz\", p ej., \n",
    "      porque los bloques regionales se borran solo si están en *cursiva*.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    out = text\n",
    "\n",
    "    out = remove_citations(out)\n",
    "\n",
    "    # Eliminación iterativa de bloques regionales en cursiva\n",
    "    while True:\n",
    "        new_out = italic_region_block.sub(\"\", out)\n",
    "        if new_out == out:\n",
    "            break\n",
    "        out = new_out\n",
    "\n",
    "    out = drop_tag.sub(\"\", out)\n",
    "\n",
    "    # Limpieza de puntuación redundante y normalización de espacios\n",
    "    out = re.sub(r\"\\s*,\\s*(?=[.,;:])\", \"\", out)\n",
    "    out = re.sub(r\"\\s*[.,;:—–-]\\s*(?=[.,;:—–-])\", \" \", out)\n",
    "    out = re.sub(r\"\\s{2,}\", \" \", out)\n",
    "    out = norm_spaces(strip_punct(out))\n",
    "    return out\n",
    "\n",
    "# ============================================================================\n",
    "# DETECCIÓN Y PROCESAMIENTO DE ACEPCIONES ENUMERADAS\n",
    "# ============================================================================\n",
    "\n",
    "bold_number_pattern = re.compile(r\"\\*\\*\\s*(\\d+)\\.\\s*\\*\\*\")\n",
    "plain_number_pattern = re.compile(r\"(?<!\\d)(\\d{1,2})\\.\\s\")\n",
    "\n",
    "def count_enumerations(full_text_after_head: str) -> int:\n",
    "    \"\"\"\n",
    "    Cuenta cuántas definiciones enumeradas (2., 3., etc.) aparecen\n",
    "    en el texto de un artículo después del lema.\n",
    "\n",
    "    Se usa para duplicar las filas en el DataFrame por cada acepción enumerada.\n",
    "\n",
    "    Ejemplos:\n",
    "   - count_numbered_definitions(\"**2.** Segunda acepción. **3.** Tercera acepción.\") -> 2\n",
    "    - count_numbered_definitions(\"Definición simple sin numeración.\") -> 0\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for g in bold_number_pattern.findall(full_text_after_head):\n",
    "        try:\n",
    "            if int(g) >= 2: count += 1\n",
    "        except: pass\n",
    "    for g in plain_number_pattern.findall(full_text_after_head):\n",
    "        try:\n",
    "            if int(g) >= 2: count += 1\n",
    "        except: pass\n",
    "    return count\n",
    "\n",
    "enumeration_marker_pattern = re.compile(r\"(?:\\*\\*\\s*(\\d+)\\.\\s*\\*\\*|(?<!\\d)(\\d{1,2})\\.\\s)\")\n",
    "\n",
    "def extract_enumerated_senses_from_first_line(first_line: str, head_end_pos: int):\n",
    "    \"\"\"\n",
    "    Extrae y separa las definiciones enumeradas (1., 2., 3., …) de la primera línea del artículo.\n",
    "\n",
    "    Separa el texto en segmentos para cada acepción numerada, y de cada segmento\n",
    "    extrae el significado y el ejemplo usando `extract_meaning_example_from_tail`.\n",
    "\n",
    "    Parámetros:\n",
    "\n",
    "    first_line : str\n",
    "        Primera línea completa del artículo (incluyendo el lema y las definiciones).\n",
    "    head_end_pos : int\n",
    "        Índice en el que termina el lema en la línea.\n",
    "\n",
    "    Retorna:\n",
    "\n",
    "    list of tuple(str, str)\n",
    "        Lista de tuplas (significado, ejemplo) para cada acepción.\n",
    "\n",
    "    Ejemplos:\n",
    "    \n",
    "     - first_line = \"- **carramán** m. coloq. Vehículo viejo. **2.** Persona vieja, acabada.\"\n",
    "     -> split_enumerated_definitions_from_line(first_line, 11)\n",
    "    [('Vehículo viejo.', 'Persona vieja, acabada.')]\n",
    "\n",
    "     - first_line = \"- **carreta** f. Carrete para hilos. **2.** Carretilla para materiales. **3.** Charla trivial.\"\n",
    "     -> split_enumerated_definitions_from_line(first_line, 10)\n",
    "    [('Carrete para hilos.', ''), ('Carretilla para materiales.', ''), ('Charla trivial.', '')]\n",
    "    \"\"\"\n",
    "    tail_raw = (first_line or \"\")[head_end_pos:]\n",
    "    tail = tail_raw\n",
    "    cuts = []\n",
    "    for m in enumeration_marker_pattern.finditer(tail):\n",
    "        num = m.group(1) or m.group(2)\n",
    "        try:\n",
    "            if int(num) >= 2:\n",
    "                cuts.append((m.start(), m.end()))\n",
    "        except:\n",
    "            pass\n",
    "    segments = []\n",
    "    if not cuts:\n",
    "        segments = [tail]\n",
    "    else:\n",
    "        start = 0\n",
    "        for (s, e) in cuts:\n",
    "            segments.append(tail[start:s])\n",
    "            start = e\n",
    "        segments.append(tail[start:])\n",
    "    senses = []\n",
    "    for seg in segments:\n",
    "        sig, ej, regions = extract_meaning_example_from_tail(seg)\n",
    "        senses.append((sig, ej, regions))\n",
    "    return senses\n",
    "\n",
    "# ============================================================================\n",
    "# UTILIDADES PARA EXTRACCIÓN DE COMPONENTES DEL LEMA\n",
    "# ============================================================================\n",
    "\n",
    "def extract_base_from_head(head_bold_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae la parte base del lema que aparece ANTES del primer '||'.\n",
    "\n",
    "    Se utiliza para normalizar la palabra principal,\n",
    "    removiendo puntuación, cursivas y sufijos de género.\n",
    "\n",
    "    Ejemplos:\n",
    "    \n",
    "    - extract_base_from_head(\"carrera || ~ de encostalados\")\n",
    "     ->'carrera'\n",
    "\n",
    "    - extract_base_from_head(\"carranchil (carranchín)\")\n",
    "     ->'carranchil'\n",
    "    \"\"\"\n",
    "    left = head_bold_content.split(\"||\", 1)[0] if \"||\" in head_bold_content else head_bold_content\n",
    "    base = strip_punct(clean_curs(norm_spaces(left)))\n",
    "    base = remove_gender_suffix(base)\n",
    "    base = strip_punct(base)\n",
    "    return norm_spaces(base)\n",
    "\n",
    "\n",
    "def check_meaning_before_pipes(first_line: str, head_end_pos: int) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica si hay contenido significativo (significado) ANTES del primer '||'\n",
    "    en el texto que sigue al lema.\n",
    "\n",
    "    Esto permite detectar si el lema principal tiene una definición propia,\n",
    "    antes de listar ocurrencias derivadas con '||'.\n",
    "\n",
    "    Ejemplo:\n",
    "    \n",
    "    - check_meaning_before_pipes(\"- **carraca** f. Mandíbula. || echar ~.\", 11)\n",
    "     -> True  # Tiene definición antes de '||'\n",
    "\n",
    "    - check_meaning_before_pipes(\"- **carrera || ~ de encostalados** Competencia...\", 7)\n",
    "     -> False # No hay definición antes de '||'\n",
    "    \"\"\"\n",
    "    tail = first_line[head_end_pos:]\n",
    "    before = tail.split(\"||\", 1)[0]\n",
    "    clean = clean_regional_blocks_and_grammar_tags(before)\n",
    "    clean = strip_punct(norm_spaces(clean))\n",
    "    return bool(clean)\n",
    "\n",
    "parenthetical_variants_pattern = re.compile(r\"\\(([^)]+)\\)\")\n",
    "\n",
    "def extract_parenthetical_variants(head_bold_content: str) -> list:\n",
    "    \"\"\"\n",
    "    Extrae las variantes indicadas entre paréntesis en el lema principal.\n",
    "\n",
    "    Ignora los casos en los que el paréntesis solo indica plural opcional (s/es).\n",
    "\n",
    "    Ejemplos:\n",
    "    \n",
    "    - extract_variants_from_parentheses(\"carranchil (carranchín)\")\n",
    "      -> ['carranchín']\n",
    "\n",
    "    - extract_variants_from_parentheses(\"carriel(es)\")\n",
    "      ->  # Ignora plural opcional\n",
    "    \"\"\"\n",
    "    left = head_bold_content.split(\"||\", 1)[0] if \"||\" in head_bold_content else head_bold_content\n",
    "    m = parenthetical_variants_pattern.search(left)\n",
    "    if not m:\n",
    "        return []\n",
    "    content = m.group(1)\n",
    "    if re.fullmatch(r\"(?:s|es)\", content.strip(), flags=re.IGNORECASE):\n",
    "        return []\n",
    "    parts = re.split(r\"\\s*(?:,|/|;|\\bo\\b|\\bu\\b|\\by\\b)\\s*\", content, flags=re.IGNORECASE)\n",
    "    variants = []\n",
    "    for raw in parts:\n",
    "        v = norm_spaces(raw)\n",
    "        if not v: continue\n",
    "        if re.fullmatch(r\"(?:s|es)\", v, flags=re.IGNORECASE): continue\n",
    "        v = remove_gender_suffix(v)\n",
    "        v = strip_punct(v)\n",
    "        v = remove_trailing_pipes(v)\n",
    "        v = norm_spaces(v)\n",
    "        if v: variants.append(nfc(v))\n",
    "    return variants\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACCIÓN DE SIGNIFICADO Y EJEMPLO\n",
    "# ============================================================================\n",
    "\n",
    "first_italic_pattern = re.compile(r\"\\*(.*?)\\*\")\n",
    "\n",
    "def extract_meaning_example_after_head(first_line: str, head_end_pos: int):\n",
    "    \"\"\"\n",
    "    Extrae el significado y el ejemplo de la PRIMERA LÍNEA de un artículo,\n",
    "    considerando el texto que sigue al lema.\n",
    "\n",
    "    Busca la primera frase en cursiva como el ejemplo.\n",
    "    El resto antes de la cursiva se considera el significado.\n",
    "\n",
    "    Ejemplo:\n",
    "    \n",
    "    - extract_meaning_and_example_from_headline( \"- **carraca** f. Mandíbula del hombre. *Había perdido la carraca.*\", 11)\n",
    "      -> ('Mandíbula del hombre.', 'Había perdido la carraca.')\n",
    "    \"\"\"\n",
    "    tail_raw = (first_line or \"\")[head_end_pos:]\n",
    "    \n",
    "    # Extraer regiones ANTES de limpiar metadatos\n",
    "    regions = extract_regions_from_text(tail_raw)\n",
    "    \n",
    "    tail = clean_regional_blocks_and_grammar_tags(tail_raw)\n",
    "    m = first_italic_pattern.search(tail)\n",
    "    if m:\n",
    "        significado = norm_spaces(strip_punct(tail[:m.start()]))\n",
    "        ejemplo = norm_spaces(strip_punct(m.group(1)))\n",
    "    else:\n",
    "        significado = norm_spaces(strip_punct(tail))\n",
    "        ejemplo = \"\"\n",
    "    significado = clean_regional_blocks_and_grammar_tags(significado)\n",
    "    ejemplo = clean_regional_blocks_and_grammar_tags(ejemplo)\n",
    "    return significado, ejemplo, regions\n",
    "\n",
    "def extract_meaning_example_from_tail(tail_raw: str):\n",
    "    \"\"\"\n",
    "    Extrae el significado y el ejemplo desde el resto de texto de un artículo\n",
    "    (sin el lema).\n",
    "\n",
    "    Busca el primer texto en cursiva como ejemplo. Lo anterior es el significado.\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    - extract_meaning_and_example_from_tail( \"Mandíbula del hombre. *Había perdido la carraca.*\")\n",
    "      -> ('Mandíbula del hombre.', 'Había perdido la carraca.')\n",
    "    \"\"\"\n",
    "    # Extraer regiones ANTES de limpiar metadatos\n",
    "    regions = extract_regions_from_text(tail_raw or \"\")\n",
    "    \n",
    "    tail = clean_regional_blocks_and_grammar_tags(tail_raw or \"\")\n",
    "    m = first_italic_pattern.search(tail)\n",
    "    if m:\n",
    "        significado = norm_spaces(strip_punct(tail[:m.start()]))\n",
    "        ejemplo = norm_spaces(strip_punct(m.group(1)))\n",
    "    else:\n",
    "        significado = norm_spaces(strip_punct(tail))\n",
    "        ejemplo = \"\"\n",
    "    significado = clean_regional_blocks_and_grammar_tags(significado)\n",
    "    ejemplo = clean_regional_blocks_and_grammar_tags(ejemplo)\n",
    "    return significado, ejemplo, regions\n",
    "\n",
    "# ============================================================================\n",
    "# SEGMENTACIÓN DEL ARCHIVO EN ARTÍCULOS\n",
    "# ============================================================================\n",
    "\n",
    "article_start_regex = re.compile(r\"^\\s*-?\\s*\\*\\*\")\n",
    "articles, current = [], []\n",
    "with open(path_source, \"r\", encoding=\"utf-8\") as f:\n",
    "    for raw in f:\n",
    "        line = nfc(raw.rstrip(\"\\n\"))\n",
    "        if article_start_regex.match(line):\n",
    "            if current:\n",
    "                articles.append(\"\\n\".join(current))\n",
    "                current = []\n",
    "        current.append(line) \n",
    "    if current:\n",
    "        articles.append(\"\\n\".join(current))\n",
    "\n",
    "# ============================================================================\n",
    "# REESCRITURA DE REFERENCIAS \"Véase **X**\"\n",
    "# ============================================================================\n",
    "\n",
    "see_also_pattern = re.compile(r\"\\bVéase\\b\\s*\\*{2}\\s*([^*]+?)\\s*\\*{2}\", flags=re.IGNORECASE)\n",
    "\n",
    "def _first_line_tail_after_head(first_line: str) -> str:\n",
    "    \"\"\"\n",
    "    Devuelve el texto que sigue inmediatamente después del bloque de cabecera en **negrita**\n",
    "    dentro de la PRIMERA LÍNEA de un artículo.\n",
    "\n",
    "    Se asume que la cabecera ya cumple el formato \"- **lema** ...\".\n",
    "    Si no hay cabecera detectada, devuelve cadena vacía.\n",
    "\n",
    "    Ejemplos:\n",
    "    \n",
    "    - get_tail_after_head_from_first_line(\"- **carraca** f. Mandíbula. *Ejemplo*\")\n",
    "      -> \" f. Mandíbula. *Ejemplo*\"\n",
    "\n",
    "    - get_tail_after_head_from_first_line(\"- **carreta || echar ~** fr. coloq. Hablar cosas triviales.\")\n",
    "       -> \" fr. coloq. Hablar cosas triviales.\"\n",
    "    \"\"\"\n",
    "    m = re_bold_head.search(first_line or \"\")\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    return (first_line or \"\")[m.end():]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754245f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONSTRUCCIÓN DE ÍNDICE DE LEMAS Y RESOLUCIÓN DE REFERENCIAS CRUZADAS\n",
    "# ============================================================================\n",
    "\n",
    "# Construcción de índice: mapea cada lema normalizado al texto que sigue\n",
    "# a su encabezado en negrita, necesario para resolver referencias \"Véase **X**\"\n",
    "lemma_to_tail = {}\n",
    "for art in articles:\n",
    "    fl = art.split(\"\\n\", 1)[0]\n",
    "    head = extract_bold_head_general(fl)\n",
    "    if not head:\n",
    "        continue\n",
    "    lema_norm = normalize_headword_content(head)\n",
    "    # Almacenar solo la primera ocurrencia de cada lema\n",
    "    if lema_norm and lema_norm not in lemma_to_tail:\n",
    "        lemma_to_tail[lema_norm] = _first_line_tail_after_head(fl)\n",
    "\n",
    "# Resolución de referencias cruzadas \"Véase **X**\"\n",
    "# Reemplaza artículos con \"Véase **palabra**\" por el contenido completo de la palabra referenciada\n",
    "rewritten_articles = []\n",
    "for art in articles:\n",
    "    parts = art.split(\"\\n\", 1)\n",
    "    fl = parts[0]\n",
    "    rest = parts[1] if len(parts) > 1 else \"\"\n",
    "    mh = re_bold_head.search(fl)\n",
    "    if not mh:\n",
    "        rewritten_articles.append(art)\n",
    "        continue\n",
    "\n",
    "    head_end = mh.end()\n",
    "    tail = fl[head_end:]\n",
    "\n",
    "    # Detectar patrón \"Véase **lema_referenciado**\"\n",
    "    msee = see_also_pattern.search(tail)\n",
    "    if not msee:\n",
    "        rewritten_articles.append(art)\n",
    "        continue\n",
    "\n",
    "    # Extraer y normalizar el lema referenciado\n",
    "    target_raw = msee.group(1)\n",
    "    target_norm = normalize_headword_content(target_raw)\n",
    "    target_tail = lemma_to_tail.get(target_norm, \"\")\n",
    "\n",
    "    # Si no se encuentra la referencia, mantener el artículo original\n",
    "    if not target_tail:\n",
    "        rewritten_articles.append(art)\n",
    "        continue\n",
    "\n",
    "    # Reemplazar \"Véase **X**\" con la definición completa de X\n",
    "    new_first_line = fl[:head_end] + \" \" + target_tail.strip()\n",
    "    new_art = new_first_line if not rest else (new_first_line + \"\\n\" + rest)\n",
    "    rewritten_articles.append(new_art)\n",
    "\n",
    "# Actualizar lista de artículos con referencias resueltas\n",
    "articles = rewritten_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330ffd7",
   "metadata": {},
   "source": [
    "## Lógica de Extracción: Requisitos 1 y 2\n",
    "\n",
    "El pipeline clasifica cada artículo según su estructura para determinar cómo extraer significado y ejemplo:\n",
    "\n",
    "**Requisito 1 (Artículo Simple):**\n",
    "- Se aplica cuando: sin pipes (||), sin numeraciones (2., 3., ...)\n",
    "- Acción: significado y ejemplo de la primera línea → asignados al lema base y variantes\n",
    "\n",
    "**Requisito 2 (Artículo con Derivados):**\n",
    "- Se aplica cuando: sin numeraciones, pero con pipes (||) en lema o como ocurrencias externas\n",
    "- Acción: significado y ejemplo de la primera línea → asignados a cada frase derivada (|| ~)\n",
    "- Caso especial: si el lema tiene significado antes del primer ||, también genera fila para el lema base\n",
    "\n",
    "**Artículos Enumerados:**\n",
    "- Se aplica cuando: contiene numeraciones (2., 3., ...)\n",
    "- Acción: ignora Req. 1 y 2, procesa cada acepción enumerada independientemente\n",
    "\n",
    "Esta clasificación evita lógica innecesariamente compleja en artículos simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d31ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE PRINCIPAL: TRANSFORMACIÓN DE ARTÍCULOS A FILAS DEL DATASET\n",
    "# ============================================================================\n",
    "\n",
    "# Recorre cada artículo preprocesado y construye filas (palabra, significado, ejemplo, región)\n",
    "# para el DataFrame final. Aplica lógica diferenciada según estructura del artículo:\n",
    "# - Artículos simples (Req. 1): una entrada directa por lema\n",
    "# - Artículos con derivados (Req. 2): múltiples entradas expandiendo pipes y tildes\n",
    "# - Artículos enumerados: una entrada por cada acepción numerada (2., 3., ...)\n",
    "#\n",
    "# Operaciones aplicadas a todos los artículos:\n",
    "# - Extracción de regiones (ANTES de eliminar metadatos en cursiva)\n",
    "# - Eliminación de etiquetas gramaticales (m., f., adj., etc.)\n",
    "# - Eliminación de citas bibliográficas\n",
    "# - Normalización Unicode (NFC) y limpieza de puntuación/espacios\n",
    "\n",
    "rows = []\n",
    "\n",
    "for art in articles:\n",
    "    first_line = art.split(\"\\n\", 1)[0]\n",
    "\n",
    "    # Extracción y validación del lema en negrita\n",
    "    head_content = extract_bold_head_general(first_line)\n",
    "    if not head_content:\n",
    "        continue\n",
    "\n",
    "    # Normalización del lema (resuelve pipes/tilde, elimina sufijos, limpia puntuación)\n",
    "    palabra_head = normalize_headword_content(head_content)\n",
    "    if not palabra_head:\n",
    "        continue\n",
    "\n",
    "    # Localización del fin del encabezado para segmentar el texto\n",
    "    m_head = re_bold_head.search(first_line)\n",
    "    head_end = m_head.end() if m_head else 0\n",
    "\n",
    "    # Texto completo después del lema (para análisis de estructura)\n",
    "    after_head_all = art[art.find(first_line) + head_end:]\n",
    "\n",
    "    # Análisis de la estructura del artículo\n",
    "    has_external_pipes = (\"||\" in after_head_all)\n",
    "    head_has_meaning = check_meaning_before_pipes(first_line, head_end) if m_head else False\n",
    "    n_extra_defs = count_enumerations(after_head_all)\n",
    "    n_defs = 1 + n_extra_defs\n",
    "    variants = extract_parenthetical_variants(head_content)\n",
    "\n",
    "    # Clasificación según Requisitos 1 y 2\n",
    "    head_has_pipes = (\"||\" in head_content)\n",
    "    is_req1_simple = (not has_external_pipes) and (n_extra_defs == 0) and (not head_has_pipes)\n",
    "    is_req2_simple = (n_extra_defs == 0) and (head_has_pipes or has_external_pipes)\n",
    "\n",
    "    # Extracción de significado/ejemplo/regiones según clasificación (Req. 1 o Req. 2)\n",
    "    sig_req1 = ej_req1 = \"\"\n",
    "    regions_req1 = []\n",
    "    sig_req2 = ej_req2 = \"\"\n",
    "    regions_req2 = []\n",
    "    if (m_head and is_req1_simple) or (m_head and is_req2_simple):\n",
    "        sig, ej, regions = extract_meaning_example_after_head(first_line, head_end)\n",
    "        if is_req1_simple:\n",
    "            sig_req1, ej_req1, regions_req1 = sig, ej, regions\n",
    "        if is_req2_simple:\n",
    "            sig_req2, ej_req2, regions_req2 = sig, ej, regions\n",
    "\n",
    "    # Procesamiento de acepciones enumeradas\n",
    "    enumerated_senses = []\n",
    "    if n_extra_defs > 0:\n",
    "        enumerated_senses = extract_enumerated_senses_from_first_line(first_line, head_end)\n",
    "        # Relleno de segmentos faltantes si la detección fue incompleta\n",
    "        if len(enumerated_senses) < n_defs:\n",
    "            enumerated_senses += [(\"\", \"\", [])] * (n_defs - len(enumerated_senses))\n",
    "\n",
    "    # Creación de filas para lema base y variantes\n",
    "    # Excepción: si hay pipes externos sin significado propio, solo se procesan derivados\n",
    "    if not (has_external_pipes and not head_has_meaning):\n",
    "        for i in range(n_defs):\n",
    "            if n_extra_defs > 0:\n",
    "                sig_i, ej_i, regions_i = enumerated_senses[i]\n",
    "            else:\n",
    "                sig_i = (sig_req1 if is_req1_simple else (sig_req2 if is_req2_simple else \"\"))\n",
    "                ej_i  = (ej_req1  if is_req1_simple else (ej_req2  if is_req2_simple else \"\"))\n",
    "                regions_i = (regions_req1 if is_req1_simple else (regions_req2 if is_req2_simple else []))\n",
    "            rows.append({\n",
    "                \"palabra\": palabra_head,\n",
    "                \"significado\": sig_i,\n",
    "                \"ejemplo\": ej_i,\n",
    "                \"región\": \", \".join(regions_i) if regions_i else \"\"\n",
    "            })\n",
    "        \n",
    "        # Filas para variantes del lema\n",
    "        for var in variants:\n",
    "            for i in range(n_defs):\n",
    "                if n_extra_defs > 0:\n",
    "                    sig_i, ej_i, regions_i = enumerated_senses[i]\n",
    "                else:\n",
    "                    sig_i = (sig_req1 if is_req1_simple else (sig_req2 if is_req2_simple else \"\"))\n",
    "                    ej_i  = (ej_req1  if is_req1_simple else (ej_req2  if is_req2_simple else \"\"))\n",
    "                    regions_i = (regions_req1 if is_req1_simple else (regions_req2 if is_req2_simple else []))\n",
    "                rows.append({\n",
    "                    \"palabra\": var,\n",
    "                    \"significado\": sig_i,\n",
    "                    \"ejemplo\": ej_i,\n",
    "                    \"región\": \", \".join(regions_i) if regions_i else \"\"\n",
    "                })\n",
    "\n",
    "    # Creación de filas para ocurrencias externas (derivados con || **...~...**)\n",
    "    if has_external_pipes:\n",
    "        base_raw = extract_base_from_head(head_content)\n",
    "\n",
    "        for bold_phrase, local_tail in external_occurrence_pattern.findall(after_head_all):\n",
    "            # Expansión de tilde y limpieza\n",
    "            phrase = replace_tilde_with_base(bold_phrase, base_raw)\n",
    "            phrase = clean_curs(phrase)\n",
    "            phrase = strip_punct(remove_trailing_pipes(norm_spaces(phrase))).rstrip(\".\")\n",
    "            if not phrase:\n",
    "                continue\n",
    "\n",
    "            # Extracción de significado/ejemplo/regiones si no hay enumeraciones\n",
    "            sig_loc = ej_loc = \"\"\n",
    "            regions_loc = []\n",
    "            if n_extra_defs == 0:\n",
    "                sig_loc, ej_loc, regions_loc = extract_meaning_example_from_tail(local_tail)\n",
    "\n",
    "            rows.append({\n",
    "                \"palabra\": phrase,\n",
    "                \"significado\": sig_loc,\n",
    "                \"ejemplo\": ej_loc,\n",
    "                \"región\": \", \".join(regions_loc) if regions_loc else \"\"\n",
    "            })\n",
    "\n",
    "# Construcción del DataFrame final\n",
    "df = pd.DataFrame(rows).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b61a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JSON guardado en: ../BDC.json\n",
      "Total de entradas: 2693\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LIMPIEZA FINAL Y EXPORTACIÓN A JSON\n",
    "# ============================================================================\n",
    "\n",
    "out_file_json = \"../BDC.json\"\n",
    "\n",
    "# Limpieza y normalización de columnas de texto\n",
    "for col in ['palabra', 'significado', 'ejemplo']:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].str.replace(r'^[\\'\"]|[\\'\"]$', '', regex=True)\n",
    "    df[col] = df[col].str.lower()\n",
    "\n",
    "# Normalización de columna de regiones (mantiene capitalización original)\n",
    "df['región'] = df['región'].astype(str).str.strip()\n",
    "\n",
    "# Ordenamiento alfabético por palabra\n",
    "df = df.sort_values(by='palabra', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Exportación a JSON\n",
    "data_json = df.to_dict(orient='records')\n",
    "\n",
    "with open(out_file_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nJSON guardado en: {out_file_json}\")\n",
    "print(f\"Total de entradas: {len(df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
