{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b10f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta de entrada: /\n",
      "Carpeta de salida: Clean/\n",
      "Modelos: ['amazon/nova-micro-v1', 'microsoft/phi-4', 'amazon/nova-lite-v1', 'cohere/command-r-08-2024', 'qwen/qwen-2.5-72b-instruct', 'google/gemma-2-27b-it', 'meta-llama/llama-3.3-70b-instruct', 'microsoft/wizardlm-2-8x22b', 'meta-llama/llama-4-maverick', 'qwen/qwen2.5-vl-32b-instruct:free', 'x-ai/grok-3-mini-beta', 'perplexity/sonar', 'mistralai/mistral-medium-3', 'mistralai/mixtral-8x7b-instruct', 'google/gemini-2.5-flash', 'meta-llama/llama-3.1-405b-instruct', 'deepseek/deepseek-chat-v3.1', 'moonshotai/kimi-k2-0905', 'openai/o4-mini-high', 'openai/gpt-4.1', 'openai/o1-mini', 'anthropic/claude-sonnet-4', 'gpt-5.1']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Configuración\n",
    "STRAICO_DIR = ''\n",
    "OUTPUT_DIR = 'Clean'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Cargar modelos desde el archivo text_model_ids.txt\n",
    "MODELS_FILE = 'models.txt'\n",
    "\n",
    "def load_models_from_file(filepath):\n",
    "    \"\"\"Carga los nombres de modelos desde un archivo de texto.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo {filepath}\")\n",
    "        return []\n",
    "\n",
    "DEFAULT_MODELS = load_models_from_file(MODELS_FILE)\n",
    "\n",
    "print(f\"Carpeta de entrada: {STRAICO_DIR}/\")\n",
    "print(f\"Carpeta de salida: {OUTPUT_DIR}/\")\n",
    "print(f\"Modelos: {DEFAULT_MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a9a02",
   "metadata": {},
   "source": [
    "## Cargar Dataset Original (Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23532e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 6533 modismos únicos\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_ground_truth(json_path: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"Carga el dataset original y elimina duplicados.\n",
    "    \n",
    "    Returns:\n",
    "        Dict con modismo como clave y dict con {significado, ejemplo} como valor\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # Si es una lista de objetos\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                modismo = (item.get('modismo') or '').strip()\n",
    "                if modismo and modismo not in ground_truth:\n",
    "                    ground_truth[modismo] = {\n",
    "                        'significado': (item.get('significado') or '').strip(),\n",
    "                        'ejemplo': (item.get('ejemplo') or '').strip()\n",
    "                    }\n",
    "        # Si es un dict directo\n",
    "        elif isinstance(data, dict):\n",
    "            for modismo, value in data.items():\n",
    "                modismo = modismo.strip()\n",
    "                if modismo and modismo not in ground_truth:\n",
    "                    ground_truth[modismo] = {\n",
    "                        'significado': (value.get('significado') or '').strip(),\n",
    "                        'ejemplo': (value.get('ejemplo') or '').strip()\n",
    "                    }\n",
    "    \n",
    "    print(f\"Dataset cargado: {len(ground_truth)} modismos únicos\")\n",
    "    return ground_truth\n",
    "\n",
    "# Cargar dataset\n",
    "ground_truth = load_ground_truth('../DataSet/DataSet_PrimeraOcurrencia.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06334cc8",
   "metadata": {},
   "source": [
    "## Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f535d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_responses(prompt_dir: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Carga las respuestas de todos los modelos para un prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt_dir: Directorio del prompt (ej: 'Straico/Prompt 1')\n",
    "        \n",
    "    Returns:\n",
    "        Dict con modelo como clave y lista de respuestas como valor\n",
    "    \"\"\"\n",
    "\n",
    "    all_models_path = os.path.join(prompt_dir, 'all_models.json')\n",
    "    \n",
    "    if os.path.exists(all_models_path):\n",
    "        with open(all_models_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    return {}\n",
    "\n",
    "\n",
    "def extract_output_field(response: Dict, field: str) -> str:\n",
    "    \"\"\"Extrae un campo del output de la respuesta del modelo.\n",
    "    \n",
    "    Maneja múltiples formatos:\n",
    "    1. Formato directo: {\"output\": {\"campo\": \"valor\"}}\n",
    "    2. Formato raw_response: {\"raw_response\": \"```json\\\\n{...}\\\\n```\"}\n",
    "    3. Errores: {\"error\": \"...\"}\n",
    "    \n",
    "    Args:\n",
    "        response: Dict con la respuesta completa\n",
    "        field: Campo a extraer del output\n",
    "        \n",
    "    Returns:\n",
    "        Valor del campo como string, vacío si hay error o no se encuentra\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(response, dict):\n",
    "            return ''\n",
    "        \n",
    "        # Verificar si hay error\n",
    "        if 'error' in response:\n",
    "            return ''\n",
    "        \n",
    "        # Caso 1: Formato directo con output\n",
    "        output = response.get('output', {})\n",
    "        if isinstance(output, dict) and field in output:\n",
    "            return str(output.get(field, '')).strip()\n",
    "        \n",
    "        # Caso 2: raw_response con JSON anidado\n",
    "        raw_response = response.get('raw_response', '')\n",
    "        if raw_response:\n",
    "            # Limpiar markdown code blocks\n",
    "            raw_response = raw_response.strip()\n",
    "            if raw_response.startswith('```json'):\n",
    "                raw_response = raw_response[7:]  # Remover ```json\n",
    "            if raw_response.startswith('```'):\n",
    "                raw_response = raw_response[3:]  # Remover ```\n",
    "            if raw_response.endswith('```'):\n",
    "                raw_response = raw_response[:-3]  # Remover ```\n",
    "            \n",
    "            raw_response = raw_response.strip()\n",
    "            \n",
    "            # Parsear el JSON anidado\n",
    "            try:\n",
    "                parsed = json.loads(raw_response)\n",
    "                if isinstance(parsed, dict):\n",
    "                    output = parsed.get('output', {})\n",
    "                    if isinstance(output, dict) and field in output:\n",
    "                        return str(output.get(field, '')).strip()\n",
    "            except json.JSONDecodeError:\n",
    "                return ''\n",
    "        \n",
    "        return ''\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error extrayendo campo '{field}': {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "def save_json(filepath: str, data: Any):\n",
    "    \"\"\"Guarda datos en formato JSON.\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Guardado: {filepath} ({len(data)} registros)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012bb59",
   "metadata": {},
   "source": [
    "## PROMPT 1: Modismo → Es Modismo (Sí/No)\n",
    "\n",
    "Procesa las respuestas del Prompt 1 para evaluar si los modelos identifican correctamente los modismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95cd949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESANDO PROMPT 1: Modismo → Es Modismo (Sí/No)\n",
      "============================================================\n",
      "  amazon/nova-micro-v1: 1 errores omitidos (0.0%)\n",
      "  microsoft/phi-4: 10 errores omitidos (0.2%)\n",
      "  amazon/nova-lite-v1: 3 errores omitidos (0.0%)\n",
      "  cohere/command-r-08-2024: 3 errores omitidos (0.0%)\n",
      "  qwen/qwen-2.5-72b-instruct: 708 errores omitidos (10.8%)\n",
      "  google/gemma-2-27b-it: 15 errores omitidos (0.2%)\n",
      "  meta-llama/llama-3.3-70b-instruct: 390 errores omitidos (6.0%)\n",
      "  microsoft/wizardlm-2-8x22b: 184 errores omitidos (2.8%)\n",
      "  meta-llama/llama-4-maverick: 8 errores omitidos (0.1%)\n",
      "  qwen/qwen2.5-vl-32b-instruct:free: EXCLUIDO (6533/6533 errores = 100.0%)\n",
      "  x-ai/grok-3-mini-beta: 284 errores omitidos (4.3%)\n",
      "  perplexity/sonar: 279 errores omitidos (4.3%)\n",
      "  mistralai/mistral-medium-3: 279 errores omitidos (4.3%)\n",
      "  mistralai/mixtral-8x7b-instruct: 610 errores omitidos (9.3%)\n",
      "  google/gemini-2.5-flash: 283 errores omitidos (4.3%)\n",
      "  meta-llama/llama-3.1-405b-instruct: 300 errores omitidos (4.6%)\n",
      "  deepseek/deepseek-chat-v3.1: 23 errores omitidos (0.4%)\n",
      "  moonshotai/kimi-k2-0905: 12 errores omitidos (0.2%)\n",
      "  openai/o4-mini-high: 4 errores omitidos (0.1%)\n",
      "  openai/gpt-4.1: 18 errores omitidos (0.3%)\n",
      "  anthropic/claude-sonnet-4: 2 errores omitidos (0.0%)\n",
      "✓ Guardado: Clean/prompt_1_metrics_data.json (140310 registros)\n",
      "\n",
      "✓ Procesados 140310 registros válidos\n",
      "✗ Omitidos 9949 registros con errores\n",
      "⊗ Modelos excluidos (>50% errores): qwen/qwen2.5-vl-32b-instruct:free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROCESANDO PROMPT 1: Modismo → Es Modismo (Sí/No)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt_1_responses = load_prompt_responses(os.path.join(STRAICO_DIR, 'Prompt 1'))\n",
    "\n",
    "# Procesar datos\n",
    "prompt_1_data = []\n",
    "errors_count = 0\n",
    "skipped_models = []\n",
    "excluded_models = []  # Modelos excluidos por alto % de errores\n",
    "\n",
    "for model in DEFAULT_MODELS:\n",
    "    if model not in prompt_1_responses:\n",
    "        print(f\"⚠ Advertencia: Modelo {model} no encontrado en Prompt 1\")\n",
    "        skipped_models.append(model)\n",
    "        continue\n",
    "    \n",
    "    model_responses = prompt_1_responses[model]\n",
    "    total_entries = len(model_responses)\n",
    "    model_errors = 0\n",
    "    model_data = []  # Datos temporales del modelo\n",
    "    \n",
    "    for entry in model_responses:\n",
    "        modismo = entry.get('modismo', '')\n",
    "        response = entry.get('response', {})\n",
    "        \n",
    "        # Verificar si hay error en la respuesta\n",
    "        if 'error' in response:\n",
    "            model_errors += 1\n",
    "            continue\n",
    "        \n",
    "        # Extraer respuesta (Sí/No) del output\n",
    "        es_modismo_generado = extract_output_field(response, 'es_modismo')\n",
    "        \n",
    "        # Solo agregar si se extrajo la respuesta correctamente\n",
    "        if es_modismo_generado:\n",
    "            model_data.append({\n",
    "                'modismo': modismo,\n",
    "                'es_modismo_real': 'Sí',  # Todos los del dataset son modismos\n",
    "                'modelo': model,\n",
    "                'es_modismo_generado': es_modismo_generado\n",
    "            })\n",
    "        else:\n",
    "            model_errors += 1\n",
    "    \n",
    "    # Calcular porcentaje de errores\n",
    "    error_percentage = (model_errors / total_entries * 100) if total_entries > 0 else 0\n",
    "    \n",
    "    # Solo incluir si el porcentaje de errores es <= 50%\n",
    "    if error_percentage > 50:\n",
    "        print(f\"  {model}: EXCLUIDO ({model_errors}/{total_entries} errores = {error_percentage:.1f}%)\")\n",
    "        excluded_models.append(model)\n",
    "        errors_count += model_errors\n",
    "    else:\n",
    "        # Agregar datos del modelo\n",
    "        prompt_1_data.extend(model_data)\n",
    "        errors_count += model_errors\n",
    "        if model_errors > 0:\n",
    "            print(f\"  {model}: {model_errors} errores omitidos ({error_percentage:.1f}%)\")\n",
    "\n",
    "# Guardar\n",
    "output_path = os.path.join(OUTPUT_DIR, 'prompt_1_metrics_data.json')\n",
    "save_json(output_path, prompt_1_data)\n",
    "\n",
    "print(f\"\\n✓ Procesados {len(prompt_1_data)} registros válidos\")\n",
    "print(f\"✗ Omitidos {errors_count} registros con errores\")\n",
    "if excluded_models:\n",
    "    print(f\"⊗ Modelos excluidos (>50% errores): {', '.join(excluded_models)}\")\n",
    "if skipped_models:\n",
    "    print(f\"⚠ Modelos no encontrados: {', '.join(skipped_models)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e7e67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TABLA PROMPT 1: Modismo → Es Modismo (Sí/No)\n",
      "================================================================================\n",
      "                            Modelo  Errores  Válidos Válidos/Total % Válidos\n",
      "              amazon/nova-micro-v1        1     6532     6532/6533    100.0%\n",
      "                   microsoft/phi-4       10     6523     6523/6533     99.8%\n",
      "               amazon/nova-lite-v1        3     6530     6530/6533    100.0%\n",
      "          cohere/command-r-08-2024        3     6530     6530/6533    100.0%\n",
      "        qwen/qwen-2.5-72b-instruct      708     5825     5825/6533     89.2%\n",
      "             google/gemma-2-27b-it       15     6518     6518/6533     99.8%\n",
      " meta-llama/llama-3.3-70b-instruct      390     6143     6143/6533     94.0%\n",
      "        microsoft/wizardlm-2-8x22b      184     6349     6349/6533     97.2%\n",
      "       meta-llama/llama-4-maverick        8     6525     6525/6533     99.9%\n",
      " qwen/qwen2.5-vl-32b-instruct:free     6533        0        0/6533      0.0%\n",
      "             x-ai/grok-3-mini-beta      284     6249     6249/6533     95.7%\n",
      "                  perplexity/sonar      279     6254     6254/6533     95.7%\n",
      "        mistralai/mistral-medium-3      279     6254     6254/6533     95.7%\n",
      "   mistralai/mixtral-8x7b-instruct      610     5923     5923/6533     90.7%\n",
      "           google/gemini-2.5-flash      283     6250     6250/6533     95.7%\n",
      "meta-llama/llama-3.1-405b-instruct      300     6233     6233/6533     95.4%\n",
      "       deepseek/deepseek-chat-v3.1       23     6510     6510/6533     99.6%\n",
      "           moonshotai/kimi-k2-0905       12     6521     6521/6533     99.8%\n",
      "               openai/o4-mini-high        4     6529     6529/6533     99.9%\n",
      "                    openai/gpt-4.1       18     6515     6515/6533     99.7%\n",
      "                    openai/o1-mini        0     6533     6533/6533    100.0%\n",
      "         anthropic/claude-sonnet-4        2     6531     6531/6533    100.0%\n",
      "                           gpt-5.1        0     6533     6533/6533    100.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tabla de resultados por modelo para Prompt 1\n",
    "prompt_1_stats = []\n",
    "\n",
    "for model in DEFAULT_MODELS:\n",
    "    if model not in prompt_1_responses:\n",
    "        continue\n",
    "    \n",
    "    model_responses = prompt_1_responses[model]\n",
    "    total_entries = len(model_responses)\n",
    "    model_errors = 0\n",
    "    valid_entries = 0\n",
    "    \n",
    "    for entry in model_responses:\n",
    "        response = entry.get('response', {})\n",
    "        \n",
    "        if 'error' in response:\n",
    "            model_errors += 1\n",
    "            continue\n",
    "        \n",
    "        es_modismo_generado = extract_output_field(response, 'es_modismo')\n",
    "        \n",
    "        if es_modismo_generado:\n",
    "            valid_entries += 1\n",
    "        else:\n",
    "            model_errors += 1\n",
    "    \n",
    "    prompt_1_stats.append({\n",
    "        'Modelo': model,\n",
    "        'Errores': model_errors,\n",
    "        'Válidos': valid_entries,\n",
    "        'Válidos/Total': f\"{valid_entries}/{total_entries}\",\n",
    "        '% Válidos': f\"{(valid_entries/total_entries*100):.1f}%\" if total_entries > 0 else \"0.0%\"\n",
    "    })\n",
    "\n",
    "# Crear DataFrame y mostrar\n",
    "df_prompt_1 = pd.DataFrame(prompt_1_stats)\n",
    "print(\"\\nTABLA PROMPT 1: Modismo → Es Modismo (Sí/No)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_prompt_1.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d9bb9",
   "metadata": {},
   "source": [
    "## PROMPT 2: Modismo → Definición\n",
    "\n",
    "Procesa las respuestas del Prompt 2 y genera datos para métricas comparando las definiciones generadas con el ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96b695b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESANDO PROMPT 2: Modismo → Definición\n",
      "============================================================\n",
      "  amazon/nova-micro-v1: 12 errores omitidos (0.2%)\n",
      "  microsoft/phi-4: 381 errores omitidos (5.8%)\n",
      "  amazon/nova-lite-v1: 75 errores omitidos (1.1%)\n",
      "  cohere/command-r-08-2024: 357 errores omitidos (5.5%)\n",
      "  qwen/qwen-2.5-72b-instruct: 733 errores omitidos (11.2%)\n",
      "  google/gemma-2-27b-it: 153 errores omitidos (2.3%)\n",
      "  meta-llama/llama-3.3-70b-instruct: 378 errores omitidos (5.8%)\n",
      "  microsoft/wizardlm-2-8x22b: 150 errores omitidos (2.3%)\n",
      "  meta-llama/llama-4-maverick: 30 errores omitidos (0.5%)\n",
      "  qwen/qwen2.5-vl-32b-instruct:free: EXCLUIDO (6533/6533 errores = 100.0%)\n",
      "  x-ai/grok-3-mini-beta: 280 errores omitidos (4.3%)\n",
      "  perplexity/sonar: 354 errores omitidos (5.4%)\n",
      "  mistralai/mistral-medium-3: 287 errores omitidos (4.4%)\n",
      "  mistralai/mixtral-8x7b-instruct: 299 errores omitidos (4.6%)\n",
      "  google/gemini-2.5-flash: 287 errores omitidos (4.4%)\n",
      "  meta-llama/llama-3.1-405b-instruct: 702 errores omitidos (10.7%)\n",
      "  deepseek/deepseek-chat-v3.1: 24 errores omitidos (0.4%)\n",
      "  moonshotai/kimi-k2-0905: 2839 errores omitidos (43.5%)\n",
      "  openai/o4-mini-high: 3 errores omitidos (0.0%)\n",
      "  openai/gpt-4.1: 10 errores omitidos (0.2%)\n",
      "  openai/o1-mini: 1 errores omitidos (0.0%)\n",
      "  gpt-5.1: 2 errores omitidos (0.0%)\n",
      "✓ Guardado: Clean/prompt_2_metrics_data.json (136369 registros)\n",
      "\n",
      "✓ Procesados 136369 registros válidos\n",
      "✗ Omitidos 13890 registros con errores\n",
      "⊗ Modelos excluidos (>50% errores): qwen/qwen2.5-vl-32b-instruct:free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROCESANDO PROMPT 2: Modismo → Definición\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt_2_responses = load_prompt_responses(os.path.join(STRAICO_DIR, 'Prompt 2'))\n",
    "\n",
    "# Procesar datos\n",
    "prompt_2_data = []\n",
    "errors_count = 0\n",
    "skipped_models = []\n",
    "excluded_models = []  # Modelos excluidos por alto % de errores\n",
    "\n",
    "for model in DEFAULT_MODELS:\n",
    "    if model not in prompt_2_responses:\n",
    "        print(f\"⚠ Advertencia: Modelo {model} no encontrado en Prompt 2\")\n",
    "        skipped_models.append(model)\n",
    "        continue\n",
    "    \n",
    "    model_responses = prompt_2_responses[model]\n",
    "    total_entries = len(model_responses)\n",
    "    model_errors = 0\n",
    "    model_data = []  # Datos temporales del modelo\n",
    "    \n",
    "    for entry in model_responses:\n",
    "        modismo = entry.get('modismo', '')\n",
    "        response = entry.get('response', {})\n",
    "        \n",
    "        # Verificar si hay error en la respuesta\n",
    "        if 'error' in response:\n",
    "            model_errors += 1\n",
    "            continue\n",
    "        \n",
    "        # Extraer definición generada del output\n",
    "        definicion_generada = extract_output_field(response, 'definicion')\n",
    "        \n",
    "        # Buscar en ground truth\n",
    "        gt = ground_truth.get(modismo, {})\n",
    "        definicion_real = gt.get('significado', '')\n",
    "        \n",
    "        # Solo agregar si se extrajo la definición correctamente\n",
    "        if definicion_generada:\n",
    "            model_data.append({\n",
    "                'modismo': modismo,\n",
    "                'definicion_real': definicion_real,\n",
    "                'modelo': model,\n",
    "                'definicion_generada': definicion_generada\n",
    "            })\n",
    "        else:\n",
    "            model_errors += 1\n",
    "    \n",
    "    # Calcular porcentaje de errores\n",
    "    error_percentage = (model_errors / total_entries * 100) if total_entries > 0 else 0\n",
    "    \n",
    "    # Solo incluir si el porcentaje de errores es <= 50%\n",
    "    if error_percentage > 50:\n",
    "        print(f\"  {model}: EXCLUIDO ({model_errors}/{total_entries} errores = {error_percentage:.1f}%)\")\n",
    "        excluded_models.append(model)\n",
    "        errors_count += model_errors\n",
    "    else:\n",
    "        # Agregar datos del modelo\n",
    "        prompt_2_data.extend(model_data)\n",
    "        errors_count += model_errors\n",
    "        if model_errors > 0:\n",
    "            print(f\"  {model}: {model_errors} errores omitidos ({error_percentage:.1f}%)\")\n",
    "\n",
    "# Guardar\n",
    "output_path = os.path.join(OUTPUT_DIR, 'prompt_2_metrics_data.json')\n",
    "save_json(output_path, prompt_2_data)\n",
    "\n",
    "print(f\"\\n✓ Procesados {len(prompt_2_data)} registros válidos\")\n",
    "print(f\"✗ Omitidos {errors_count} registros con errores\")\n",
    "if excluded_models:\n",
    "    print(f\"⊗ Modelos excluidos (>50% errores): {', '.join(excluded_models)}\")\n",
    "if skipped_models:\n",
    "    print(f\"⚠ Modelos no encontrados: {', '.join(skipped_models)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94f5908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TABLA PROMPT 2: Modismo → Definición\n",
      "================================================================================\n",
      "                            Modelo  Errores  Válidos Válidos/Total % Válidos\n",
      "              amazon/nova-micro-v1       12     6521     6521/6533     99.8%\n",
      "                   microsoft/phi-4      381     6152     6152/6533     94.2%\n",
      "               amazon/nova-lite-v1       75     6458     6458/6533     98.9%\n",
      "          cohere/command-r-08-2024      357     6176     6176/6533     94.5%\n",
      "        qwen/qwen-2.5-72b-instruct      733     5800     5800/6533     88.8%\n",
      "             google/gemma-2-27b-it      153     6380     6380/6533     97.7%\n",
      " meta-llama/llama-3.3-70b-instruct      378     6155     6155/6533     94.2%\n",
      "        microsoft/wizardlm-2-8x22b      150     6383     6383/6533     97.7%\n",
      "       meta-llama/llama-4-maverick       30     6503     6503/6533     99.5%\n",
      " qwen/qwen2.5-vl-32b-instruct:free     6533        0        0/6533      0.0%\n",
      "             x-ai/grok-3-mini-beta      280     6253     6253/6533     95.7%\n",
      "                  perplexity/sonar      354     6179     6179/6533     94.6%\n",
      "        mistralai/mistral-medium-3      287     6246     6246/6533     95.6%\n",
      "   mistralai/mixtral-8x7b-instruct      299     6234     6234/6533     95.4%\n",
      "           google/gemini-2.5-flash      287     6246     6246/6533     95.6%\n",
      "meta-llama/llama-3.1-405b-instruct      702     5831     5831/6533     89.3%\n",
      "       deepseek/deepseek-chat-v3.1       24     6509     6509/6533     99.6%\n",
      "           moonshotai/kimi-k2-0905     2839     3694     3694/6533     56.5%\n",
      "               openai/o4-mini-high        3     6530     6530/6533    100.0%\n",
      "                    openai/gpt-4.1       10     6523     6523/6533     99.8%\n",
      "                    openai/o1-mini        1     6532     6532/6533    100.0%\n",
      "         anthropic/claude-sonnet-4        0     6533     6533/6533    100.0%\n",
      "                           gpt-5.1        2     6531     6531/6533    100.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tabla de resultados por modelo para Prompt 2\n",
    "prompt_2_stats = []\n",
    "\n",
    "for model in DEFAULT_MODELS:\n",
    "    if model not in prompt_2_responses:\n",
    "        continue\n",
    "    \n",
    "    model_responses = prompt_2_responses[model]\n",
    "    total_entries = len(model_responses)\n",
    "    model_errors = 0\n",
    "    valid_entries = 0\n",
    "    \n",
    "    for entry in model_responses:\n",
    "        response = entry.get('response', {})\n",
    "        \n",
    "        if 'error' in response:\n",
    "            model_errors += 1\n",
    "            continue\n",
    "        \n",
    "        definicion_generada = extract_output_field(response, 'definicion')\n",
    "        \n",
    "        if definicion_generada:\n",
    "            valid_entries += 1\n",
    "        else:\n",
    "            model_errors += 1\n",
    "    \n",
    "    prompt_2_stats.append({\n",
    "        'Modelo': model,\n",
    "        'Errores': model_errors,\n",
    "        'Válidos': valid_entries,\n",
    "        'Válidos/Total': f\"{valid_entries}/{total_entries}\",\n",
    "        '% Válidos': f\"{(valid_entries/total_entries*100):.1f}%\" if total_entries > 0 else \"0.0%\"\n",
    "    })\n",
    "\n",
    "# Crear DataFrame y mostrar\n",
    "df_prompt_2 = pd.DataFrame(prompt_2_stats)\n",
    "print(\"\\nTABLA PROMPT 2: Modismo → Definición\")\n",
    "print(\"=\" * 80)\n",
    "print(df_prompt_2.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9b916",
   "metadata": {},
   "source": [
    "## PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
    "\n",
    "Procesa las respuestas del Prompt 3 para evaluar la capacidad de los modelos de generar interpretaciones literales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd7c1872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 4897 modismos únicos\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset\n",
    "ground_truth = load_ground_truth('../DataSet/DataSet_ConEjemplos.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f79ee8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESANDO PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
      "============================================================\n",
      "  amazon/nova-micro-v1: 40 errores omitidos (0.8%)\n",
      "  microsoft/phi-4: 24 errores omitidos (0.5%)\n",
      "  amazon/nova-lite-v1: 20 errores omitidos (0.4%)\n",
      "  cohere/command-r-08-2024: 24 errores omitidos (0.5%)\n",
      "  qwen/qwen-2.5-72b-instruct: 568 errores omitidos (11.6%)\n",
      "  google/gemma-2-27b-it: 84 errores omitidos (1.7%)\n",
      "  meta-llama/llama-3.3-70b-instruct: 114 errores omitidos (2.3%)\n",
      "  microsoft/wizardlm-2-8x22b: EXCLUIDO (4895/4897 errores = 100.0%)\n",
      "  meta-llama/llama-4-maverick: 28 errores omitidos (0.6%)\n",
      "  qwen/qwen2.5-vl-32b-instruct:free: EXCLUIDO (4897/4897 errores = 100.0%)\n",
      "  x-ai/grok-3-mini-beta: 291 errores omitidos (5.9%)\n",
      "  perplexity/sonar: 3 errores omitidos (0.1%)\n",
      "  mistralai/mistral-medium-3: 154 errores omitidos (3.1%)\n",
      "  mistralai/mixtral-8x7b-instruct: 395 errores omitidos (8.1%)\n",
      "  google/gemini-2.5-flash: 295 errores omitidos (6.0%)\n",
      "  meta-llama/llama-3.1-405b-instruct: 406 errores omitidos (8.3%)\n",
      "  deepseek/deepseek-chat-v3.1: 321 errores omitidos (6.6%)\n",
      "  moonshotai/kimi-k2-0905: 234 errores omitidos (4.8%)\n",
      "  openai/o4-mini-high: 160 errores omitidos (3.3%)\n",
      "  openai/gpt-4.1: 18 errores omitidos (0.4%)\n",
      "  openai/o1-mini: 6 errores omitidos (0.1%)\n",
      "  anthropic/claude-sonnet-4: 141 errores omitidos (2.9%)\n",
      "  gpt-5.1: 4 errores omitidos (0.1%)\n",
      "✓ Guardado: Clean/prompt_3_metrics_data.json (99507 registros)\n",
      "\n",
      "✓ Procesados 99507 registros válidos\n",
      "✗ Omitidos 13122 registros con errores\n",
      "⊗ Modelos excluidos (>50% errores): microsoft/wizardlm-2-8x22b, qwen/qwen2.5-vl-32b-instruct:free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROCESANDO PROMPT 3: Modismo + Ejemplo → Literal + Definición\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cargar respuestas\n",
    "prompt_3_responses = load_prompt_responses(os.path.join(STRAICO_DIR, 'Prompt 3'))\n",
    "\n",
    "# Procesar datos\n",
    "prompt_3_data = []\n",
    "errors_count = 0\n",
    "skipped_models = []\n",
    "excluded_models = []  # Modelos excluidos por alto % de errores\n",
    "\n",
    "for model in DEFAULT_MODELS:\n",
    "    if model not in prompt_3_responses:\n",
    "        print(f\"⚠ Advertencia: Modelo {model} no encontrado en Prompt 3\")\n",
    "        skipped_models.append(model)\n",
    "        continue\n",
    "    \n",
    "    model_responses = prompt_3_responses[model]\n",
    "    total_entries = len(model_responses)\n",
    "    model_errors = 0\n",
    "    model_data = []  # Datos temporales del modelo\n",
    "    \n",
    "    for entry in model_responses:\n",
    "        modismo = entry.get('modismo', '')\n",
    "        ejemplo = entry.get('ejemplo', '')\n",
    "        response = entry.get('response', {})\n",
    "        \n",
    "        # Verificar que response sea un diccionario\n",
    "        if not isinstance(response, dict):\n",
    "            model_errors += 1\n",
    "            continue\n",
    "        \n",
    "        # Verificar si hay error en la respuesta\n",
    "        if 'error' in response:\n",
    "            model_errors += 1\n",
    "            continue\n",
    "        \n",
    "        # NOTA: El campo es 'sinonimo' no 'literal' en Prompt 3\n",
    "        literal_generado = extract_output_field(response, 'sinonimo')\n",
    "        definicion_generada = extract_output_field(response, 'definicion')\n",
    "        \n",
    "        # Buscar en ground truth\n",
    "        gt = ground_truth.get(modismo, {})\n",
    "        \n",
    "        # Solo agregar si se extrajo al menos un campo correctamente\n",
    "        if literal_generado or definicion_generada:\n",
    "            model_data.append({\n",
    "                'modismo': modismo,\n",
    "                'ejemplo': ejemplo,\n",
    "                'significado_real': gt.get('significado', ''),\n",
    "                'modelo': model,\n",
    "                'literal_generado': literal_generado,\n",
    "                'definicion_generada': definicion_generada\n",
    "            })\n",
    "        else:\n",
    "            model_errors += 1\n",
    "    \n",
    "    # Calcular porcentaje de errores\n",
    "    error_percentage = (model_errors / total_entries * 100) if total_entries > 0 else 0\n",
    "    \n",
    "    # Solo incluir si el porcentaje de errores es <= 50%\n",
    "    if error_percentage > 50:\n",
    "        print(f\"  {model}: EXCLUIDO ({model_errors}/{total_entries} errores = {error_percentage:.1f}%)\")\n",
    "        excluded_models.append(model)\n",
    "        errors_count += model_errors\n",
    "    else:\n",
    "        # Agregar datos del modelo\n",
    "        prompt_3_data.extend(model_data)\n",
    "        errors_count += model_errors\n",
    "        if model_errors > 0:\n",
    "            print(f\"  {model}: {model_errors} errores omitidos ({error_percentage:.1f}%)\")\n",
    "\n",
    "# Guardar\n",
    "output_path = os.path.join(OUTPUT_DIR, 'prompt_3_metrics_data.json')\n",
    "save_json(output_path, prompt_3_data)\n",
    "\n",
    "print(f\"\\n✓ Procesados {len(prompt_3_data)} registros válidos\")\n",
    "print(f\"✗ Omitidos {errors_count} registros con errores\")\n",
    "if excluded_models:\n",
    "    print(f\"⊗ Modelos excluidos (>50% errores): {', '.join(excluded_models)}\")\n",
    "if skipped_models:\n",
    "    print(f\"⚠ Modelos no encontrados: {', '.join(skipped_models)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c97d66f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TABLA PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
      "================================================================================\n",
      "                            Modelo  Errores  Válidos Válidos/Total % Válidos\n",
      "              amazon/nova-micro-v1       40     4857     4857/4897     99.2%\n",
      "                   microsoft/phi-4       24     4873     4873/4897     99.5%\n",
      "               amazon/nova-lite-v1       20     4877     4877/4897     99.6%\n",
      "          cohere/command-r-08-2024       24     4873     4873/4897     99.5%\n",
      "        qwen/qwen-2.5-72b-instruct      568     4329     4329/4897     88.4%\n",
      "             google/gemma-2-27b-it       84     4813     4813/4897     98.3%\n",
      " meta-llama/llama-3.3-70b-instruct      114     4783     4783/4897     97.7%\n",
      "        microsoft/wizardlm-2-8x22b     4895        2        2/4897      0.0%\n",
      "       meta-llama/llama-4-maverick       28     4869     4869/4897     99.4%\n",
      " qwen/qwen2.5-vl-32b-instruct:free     4897        0        0/4897      0.0%\n",
      "             x-ai/grok-3-mini-beta      291     4606     4606/4897     94.1%\n",
      "                  perplexity/sonar        3     4894     4894/4897     99.9%\n",
      "        mistralai/mistral-medium-3      154     4743     4743/4897     96.9%\n",
      "   mistralai/mixtral-8x7b-instruct      395     4502     4502/4897     91.9%\n",
      "           google/gemini-2.5-flash      295     4602     4602/4897     94.0%\n",
      "meta-llama/llama-3.1-405b-instruct      406     4491     4491/4897     91.7%\n",
      "       deepseek/deepseek-chat-v3.1      321     4576     4576/4897     93.4%\n",
      "           moonshotai/kimi-k2-0905      234     4663     4663/4897     95.2%\n",
      "               openai/o4-mini-high      160     4737     4737/4897     96.7%\n",
      "                    openai/gpt-4.1       18     4879     4879/4897     99.6%\n",
      "                    openai/o1-mini        6     4891     4891/4897     99.9%\n",
      "         anthropic/claude-sonnet-4      141     4756     4756/4897     97.1%\n",
      "                           gpt-5.1        4     4893     4893/4897     99.9%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tabla de resultados por modelo para Prompt 3\n",
    "prompt_3_stats = []\n",
    "\n",
    "for model in DEFAULT_MODELS:\n",
    "    if model not in prompt_3_responses:\n",
    "        continue\n",
    "    \n",
    "    model_responses = prompt_3_responses[model]\n",
    "    total_entries = len(model_responses)\n",
    "    model_errors = 0\n",
    "    valid_entries = 0\n",
    "    \n",
    "    for entry in model_responses:\n",
    "        response = entry.get('response', {})\n",
    "        \n",
    "        if not isinstance(response, dict):\n",
    "            model_errors += 1\n",
    "            continue\n",
    "        \n",
    "        if 'error' in response:\n",
    "            model_errors += 1\n",
    "            continue\n",
    "        \n",
    "        literal_generado = extract_output_field(response, 'sinonimo')\n",
    "        definicion_generada = extract_output_field(response, 'definicion')\n",
    "        \n",
    "        if literal_generado or definicion_generada:\n",
    "            valid_entries += 1\n",
    "        else:\n",
    "            model_errors += 1\n",
    "    \n",
    "    prompt_3_stats.append({\n",
    "        'Modelo': model,\n",
    "        'Errores': model_errors,\n",
    "        'Válidos': valid_entries,\n",
    "        'Válidos/Total': f\"{valid_entries}/{total_entries}\",\n",
    "        '% Válidos': f\"{(valid_entries/total_entries*100):.1f}%\" if total_entries > 0 else \"0.0%\"\n",
    "    })\n",
    "\n",
    "# Crear DataFrame y mostrar\n",
    "df_prompt_3 = pd.DataFrame(prompt_3_stats)\n",
    "print(\"\\nTABLA PROMPT 3: Modismo + Ejemplo → Literal + Definición\")\n",
    "print(\"=\" * 80)\n",
    "print(df_prompt_3.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e6a05",
   "metadata": {},
   "source": [
    "## Resumen de Archivos Generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73065d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESUMEN DE ARCHIVOS GENERADOS\n",
      "============================================================\n",
      "  prompt_1_metrics_data.json: 140310 registros\n",
      "  prompt_2_metrics_data.json: 136369 registros\n",
      "  prompt_3_metrics_data.json: 99507 registros\n",
      "============================================================\n",
      "Total: 3 archivos, 376186 registros\n",
      "Ubicación: Clean/\n",
      "\n",
      "✓ Procesamiento completado\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN DE ARCHIVOS GENERADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import glob\n",
    "\n",
    "# Listar todos los archivos JSON generados\n",
    "json_files = glob.glob(os.path.join(OUTPUT_DIR, '*.json'))\n",
    "json_files.sort()\n",
    "\n",
    "total_registros = 0\n",
    "for filepath in json_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    num_registros = len(data)\n",
    "    total_registros += num_registros\n",
    "    \n",
    "    print(f\"  {filename}: {num_registros} registros\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total: {len(json_files)} archivos, {total_registros} registros\")\n",
    "print(f\"Ubicación: {OUTPUT_DIR}/\")\n",
    "print(\"\\n✓ Procesamiento completado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
